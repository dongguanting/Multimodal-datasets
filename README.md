# Multimodal datasets

This repository is build in association with our position paper on "Multimodality for NLP-Centered Applications: Resources, Advances and
Frontiers".

As a part of this release we share the information about recent multimodal datasets which are available for research purposes.

We found that although 100+ multimodal language resources are available in literature for various NLP tasks, still publicly available multimodal datasets are under-explored for its re-usage in subsequent problem domains.

# Multimodal datasets for NLP Applications

1. **Sentiment Analysis**

| **Dataset**       | **Title of the Paper** | **Link of the Paper** | **Link of the Dataset** |
| ----------------- | ---------------------- |---------------------- |------------------------ |
| EmoDB| A Database of German Emotional Speech| [Paper](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.130.8506&rep=rep1&type=pdf) | https://www.kaggle.com/piyushagni5/berlin-database-of-emotional-speech-emodb | 
| VAM | The Vera am Mittag German Audio-Visual Emotional Speech Database | [Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4607572)  | https://sail.usc.edu/VAM/vam_release.htm | 
| IEMOCAP| IEMOCAP: interactive emotional dyadic motion capture database | [Paper](https://link.springer.com/content/pdf/10.1007/s10579-008-9076-6.pdf) | https://sail.usc.edu/software/databases/ | 
| Mimicry|A Multimodal Database for Mimicry Analysis| [Paper](https://ibug.doc.ic.ac.uk/media/uploads/documents/sun2011multimodal.pdf) | http://www.mahnob-db.eu/mimicry | 
| YouTube| Towards Multimodal Sentiment Analysis:Harvesting Opinions from the Web|https://ict.usc.edu/pubs/Towards%20Multimodal%20Sentiment%20Analysis-%20Harvesting%20Opinions%20from%20The%20Web.pdf | https://github.com/A2Zadeh/CMU-MultimodalSDK | 
| HUMAINE| The HUMAINE database | [Paper](https://ibug.doc.ic.ac.uk/media/uploads/documents/sun2011multimodal.pdf) | www.emotion-research.net | 
| Large Movies| Sentiment classification on Large Movie Review |https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf | https://ai.stanford.edu/~amaas/data/sentiment/ | 
| SEMAINE| The SEMAINE Database: Annotated Multimodal Records of Emotionally Colored Conversations between a Person and a Limited Agent |https://ieeexplore.ieee.org/document/5959155 | https://semaine-db.eu/| 
| AFEW| Collecting Large, Richly Annotated Facial-Expression Databases from Movies|http://users.cecs.anu.edu.au/~adhall/Dhall_Goecke_Lucey_Gedeon_M_2012.pdf | https://cs.anu.edu.au/few/AFEW.html | 
| SST| Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank |https://aclanthology.org/D13-1170.pdf | https://metatext.io/datasets/the-stanford-sentiment-treebank-(sst) | 
| ICT-MMMO| YouTube Movie Reviews: Sentiment Analysis in an AudioVisual Context|http://multicomp.cs.cmu.edu/wp-content/uploads/2017/09/2013_IEEEIS_wollmer_youtube.pdf | https://github.com/A2Zadeh/CMU-MultimodalSDK | 
| RECOLA| Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions |https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6553805 | https://diuf.unifr.ch/main/diva/recola/download.html | 
| MOUD| Utterance-Level Multimodal Sentiment Analysis |https://aclanthology.org/P13-1096.pdf | Utterance-Level Multimodal Sentiment Analysis | 
| CMU-MOSI| MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos |https://arxiv.org/ftp/arxiv/papers/1606/1606.06259.pdf | https://github.com/A2Zadeh/CMU-MultimodalSDK | 
| POM| Multimodal Analysis and Prediction of Persuasiveness in Online Social Multimedia |https://dl.acm.org/doi/pdf/10.1145/2897739 | https://github.com/eusip/POM | 
| MELD| MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations |https://arxiv.org/pdf/1810.02508.pdf | https://affective-meld.github.io/ | 
| CMU-MOSEI| Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph |https://aclanthology.org/P18-1208.pdf | https://github.com/A2Zadeh/CMU-MultimodalSDK | 
| AMMER| Towards Multimodal Emotion Recognition in German Speech Events in Cars using Transfer Learning |https://arxiv.org/pdf/1909.02764.pdf | On Request | 
| SEWA| SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild |https://arxiv.org/pdf/1901.02839.pdf | http://www.sewaproject.eu/resources | 
| Fakeddit| r/fakeddit: A new multimodal benchmark dataset for fine-grained fake news detection |https://arxiv.org/pdf/1911.03854.pdf | https://fakeddit.netlify.app/ | 
| CMU-MOSEAS| CMU-MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French |https://aclanthology.org/2020.emnlp-main.141.pdf | https://bit.ly/2Svbg9f | 
| MultiOFF| Multimodal meme dataset (MultiOFF) for identifying offensive content in image and text |https://aclanthology.org/2020.trac-1.6.pdf | https://github.com/bharathichezhiyan/Multimodal-Meme-Classification-Identifying-Offensive-Content-in-Image-and-Text | 
| MEISD| MEISD: A Multimodal Multi-Label Emotion, Intensity and Sentiment Dialogue Dataset for Emotion Recognition and Sentiment Analysis in Conversations |https://aclanthology.org/2020.coling-main.393.pdf | https://github.com/declare-lab/MELD | 
| TASS| Overview of TASS 2020: Introducing Emotion  |http://ceur-ws.org/Vol-2664/tass_overview.pdf | http://www.sepln.org/workshops/tass/tass_data/download.php | 
| CH SIMS| CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality |https://aclanthology.org/2020.acl-main.343.pdf | https://github.com/thuiar/MMSA | 
| Creep-Image| A Multimodal Dataset of Images and Text |http://ceur-ws.org/Vol-2769/paper_11.pdf | https://github.com/dhfbk/creep-image-dataset | 
| Entheos| Entheos: A Multimodal Dataset for Studying Enthusiasm |https://aclanthology.org/2021.findings-acl.180.pdf | https://github.com/clviegas/Entheos-Dataset | 
 

2. **Machine Translation**

| **Dataset**       | **Title of the Paper** | **Link of the Paper** | **Link of the Dataset** |
| ----------------- | ---------------------- |---------------------- |------------------------ |
| Multi30K| Multi30K: Multilingual English-German Image Description |https://arxiv.org/pdf/1605.00459.pdf | https://github.com/multi30k/dataset | 
| How2| How2: A Large-scale Dataset for Multimodal Language Understanding |https://arxiv.org/pdf/1811.00347.pdf | https://github.com/srvk/how2-dataset | 
| MLT | Multimodal Lexical Translation |https://aclanthology.org/L18-1602.pdf | https://github.com/sheffieldnlp/mlt | 
| IKEA | A Visual Attention Grounding Neural Model for Multimodal Machine Translation |https://arxiv.org/pdf/1808.08266.pdf | https://github.com/sampalomad/IKEA-Dataset | 
| Flickr30K (EN- (hi-IN)) | Multimodal Neural Machine Translation for Low-resource Language Pairs using Synthetic Data |https://aclanthology.org/W18-3405.pdf | On Request | 
| Hindi Visual Genome | Hindi Visual Genome: A Dataset for Multimodal English-to-Hindi Machine Translation |https://arxiv.org/pdf/1907.08948.pdf | https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2997 | 
| HowTo100M | Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models | https://arxiv.org/pdf/2103.08849.pdf | https://github.com/berniebear/Multi-HT100M | 


3. **Information Retrieval**

| **Dataset**       | **Title of the Paper** | **Link of the Paper** | **Link of the Dataset** |
| ----------------- | ---------------------- |---------------------- |------------------------ | 
| MUSICLEF | MusiCLEF: a Benchmark Activity in Multimodal Music Information Retrieval |https://ismir2011.ismir.net/papers/OS6-3.pdf | http://www.cp.jku.at/datasets/musiclef/index.html | 
| Moodo | The Moodo dataset: Integrating user context with emotional and color perception of music for affective music information retrieval |https://www.tandfonline.com/doi/pdf/10.1080/09298215.2017.1333518?casa_token=GxB97r7M-WMAAAAA:7ZfS-mY7f3WTP0FBbpiaSIdU-tcRXdIIwCiLLCG8ghkw_FTRn_Ha3cPD7s_6i29RwLBd6EPJmg | http://moodo.musiclab.si | 
| ALF-200k | ALF-200k: Towards Extensive Multimodal Analyses of Music Tracks and Playlists |https://dbis-informatik.uibk.ac.at/sites/default/files/2018-04/ecir-2018-alf.pdf | https://github.com/dbis-uibk/ALF200k | 
| MQA | Can Image Captioning Help Passage Retrieval in Multimodal Question Answering? |https://www.springerprofessional.de/en/can-image-captioning-help-passage-retrieval-in-multimodal-questi/16626696 | https://huggingface.co/datasets/clips/mqa | 
| WAT2019 | WAT2019: English-Hindi Translation on Hindi Visual Genome Dataset |https://github.com/sheffieldnlp/mlt	| https://aclanthology.org/L18-1602.pdf | 
| ViTT | Multimodal Pretraining for Dense Video Captioning |https://arxiv.org/pdf/2011.11760.pdf | https://github.com/google-research-datasets/Video-Timeline-Tags-ViTT | 
| MTD | MTD: A Multimodal Dataset of Musical Themes for MIR Research |https://transactions.ismir.net/articles/10.5334/tismir.68/ | https://www.audiolabs-erlangen.de/resources/MIR/MTD | 
| MusiClef | A professionally annotated and enriched multimodal data set on popular music|https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.302.2718&rep=rep1&type=pdf | http://www.cp.jku.at/datasets/musiclef/index.html | 
| Schubert Winterreise | Schubert Winterreise dataset: A multimodal scenario for music analysis |https://dl.acm.org/doi/pdf/10.1145/3429743 | https://zenodo.org/record/3968389#.YcQrk2hBxPY | 
| WIT | WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning |https://arxiv.org/pdf/2103.01913.pdf | https://github.com/google-research-datasets/wit | 

4. **Question Answering**

| **Dataset**       | **Title of the Paper** | **Link of the Paper** | **Link of the Dataset** |
| ----------------- | ---------------------- |---------------------- |------------------------ | 
| MQA | A Dataset for Multimodal Question Answering in the Cultural Heritage Domain | https://aclanthology.org/W16-4003.pdf | - | 
| MovieQA |Movieqa: Understanding stories in movies through question-answering	MovieQA	| https://arxiv.org/pdf/1512.02902.pdf |  https://github.com/makarandtapaswi/MovieQA_CVPR2016 |	
| PororoQA |Deep story video story qa by deep embedded memory networks|	https://arxiv.org/ftp/arxiv/papers/1707/1707.00836.pdf |	https://github.com/Kyung-Min/Deep-Embedded-Memory-Networks| 
| MemexQA |MemexQA: Visual Memex Question Answering |	https://arxiv.org/pdf/1708.01336.pdf |	https://memexqa.cs.cmu.edu/ | 
| VQA |Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering |	https://arxiv.org/pdf/1612.00837.pdf |	https://visualqa.org/| 
| TDIUC |An analysis of visual question answering algorithms	| https://openaccess.thecvf.com/content_ICCV_2017/papers/Kafle_An_Analysis_of_ICCV_2017_paper.pdf	| https://kushalkafle.com/projects/tdiuc.html | 
| TGIF-QA |TGIF-QA: Toward spatio-temporal reasoning in visual question answering |	https://arxiv.org/pdf/1704.04497.pdf |	https://github.com/YunseokJANG/tgif-qa | 
| MSVD QA, MSRVTT QA |Video question answering via attribute augmented attention network learning |	https://arxiv.org/pdf/1707.06355.pdf |	https://github.com/xudejing/video-question-answering | 
| YouTube2Text |Video Question Answering via Gradually Refined Attention over Appearance and Motion |	http://staff.ustc.edu.cn/~hexn/papers/mm17-videoQA.pdf	| https://github.com/topics/youtube2text | 
| MovieFIB |A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering |	https://arxiv.org/pdf/1611.07810.pdf |	https://github.com/teganmaharaj/MovieFIB/blob/master/README.md | 
| Video Context QA |Uncovering the temporal context for video question answering |	https://arxiv.org/pdf/1511.04670.pdf |	https://github.com/ffmpbgrnn/VideoQA | 
| MarioQA |Marioqa: Answering questions by watching gameplay videos |	https://arxiv.org/pdf/1612.01669.pdf |	https://github.com/JonghwanMun/MarioQA | 
| TVQA |Tvqa: Localized, compositional video question answering |	https://arxiv.org/pdf/1809.01696.pdf |	https://tvqa.cs.unc.edu/ | 
| VQA-CP v2 |Don’t just assume; look and answer: Overcoming priors for visual question answering |	https://arxiv.org/pdf/1712.00377.pdf |	https://github.com/cdancette/vqa-cp-leaderboard | 
| RecipeQA | RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes|https://arxiv.org/pdf/1809.00812.pdf | https://hucvl.github.io/recipeqa/ | 
| GQA |GQA: A new dataset for real-world visual reasoning and compositional question answering	| https://arxiv.org/pdf/1902.09506v3.pdf |	https://github.com/leaderj1001/Vision-Language | 
| Social IQ |Social-iq: A question answering benchmark for artificial social intelligence |	https://openaccess.thecvf.com/content_CVPR_2019/papers/Zadeh_Social-IQ_A_Question_Answering_Benchmark_for_Artificial_Social_Intelligence_CVPR_2019_paper.pdf |	https://github.com/A2Zadeh/CMU-MultimodalSDK | 
| MIMOQA |MIMOQA: Multimodal Input Multimodal Output Question Answering |	https://aclanthology.org/2021.naacl-mai | - | 


5. **Summarization**

| **Dataset**       | **Title of the Paper** | **Link of the Paper** | **Link of the Dataset** |
| ----------------- | ---------------------- |---------------------- |------------------------ |
| SumMe ||[Paper](**write the paper link here**) | [Dataset](**write the dataset link here**) | 
| TVSum ||Paper | Dataset | 
| QFVS ||Paper | Dataset | 
| Dev AM ||Paper | Dataset | 
| MMSS ||Paper | Dataset | 
| MSMO ||Paper | Dataset | 
| Screen2Words ||Paper | Dataset | 
| MM AVS ||Paper | Dataset | 
| AVIATE ||Paper | Dataset | 
| Multimodal Microblog Summarizaion ||Paper | Dataset | 


6. **Human Computer Interaction**

| **Dataset**       | **Title of the Paper** | **Link of the Paper** | **Link of the Dataset** |
| ----------------- | ---------------------- |---------------------- |------------------------ |
| CAUVE ||Paper | Dataset | 
| MHAD ||Paper | Dataset | 
| Multi-party interactions ||Paper | Dataset | 
| MHHRI ||Paper | Dataset | 
| Red Hen Lab ||Paper | Dataset | 
| EMRE ||Paper | Dataset | 
| Chinese Whispers ||Paper | Dataset | 
| uulmMAC ||Paper | Dataset | 

7. **Semantic Analysis**

| **Dataset**       | **Title of the Paper** | **Link of the Paper** | **Link of the Dataset** |
| ----------------- | ---------------------- |---------------------- |------------------------ |
| WN9-IMG ||Paper | Dataset | 
| Wikimedia Commons ||Paper | Dataset | 
| Starsem18-multimodalKB ||Paper | Dataset | 
| MUStARD ||Paper | Dataset | 
| YouMakeup ||Paper | Dataset | 
| MDID ||Paper | Dataset | 
| Social media posts from Flickr (Mental Health) ||Paper | Dataset | 
| Twitter MEL ||Paper | Dataset | 
| MultiMET ||Paper | Dataset | 
| MSDS ||Paper | Dataset | 

8. **Miscellaneous**

| **Dataset**       | **Title of the Paper** | **Link of the Paper** | **Link of the Dataset** |
| ----------------- | ---------------------- |---------------------- |------------------------ |
| MS COCO ||Paper | Dataset | 
| ILSVRC ||Paper | Dataset | 
| YFCC100M ||Paper | Dataset | 
| COGNIMUSE ||Paper | Dataset | 
| SNAG ||Paper | Dataset | 
| UR-Funny ||Paper | Dataset | 
| Bag-of-Lies ||Paper | Dataset | 
| MARC ||Paper | Dataset | 
| MuSE ||Paper | Dataset | 
| BabelPic ||Paper | Dataset | 
| Eye4Ref ||Paper | Dataset | 
| Troll Memes ||Paper | Dataset | 
| SEMD ||Paper | Dataset | 
| Chat talk Corpus ||Paper | Dataset | 
| EMOTyDA ||Paper | Dataset | 
| MELINDA ||Paper | Dataset | 
| NewsCLIPpings ||Paper | Dataset | 
| R2VQ ||Paper | Dataset | 
| M2H2 ||Paper | Dataset | 
